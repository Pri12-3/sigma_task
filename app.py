# -*- coding: utf-8 -*-
"""APP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PCga96CMC_NQg61iq2IiLVF7R0IXtb7w
"""

from google.colab import files
uploaded = files.upload()

!pip install streamlit pyngrok --quiet

# app.py

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, LabelEncoder
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from sklearn.decomposition import PCA

# Title
st.title("Customer Segmentation using Hierarchical Clustering")
df = pd.read_csv( "Customer_Segmentation_Dataset.csv")
st.subheader("Raw Dataset")
st.write(df.head())
print(df.shape)
print(df)

st.title("Customer Segmentation with Hierarchical Clustering and PCA")

# Assume df is already loaded in your environment

# Drop irrelevant columns
drop_cols = ['ID', 'Dt_Customer', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3',
             'AcceptedCmp4', 'AcceptedCmp5', 'Response']
df = df.drop(columns=drop_cols, errors='ignore')

# Create Age from Year_Birth
df['Age'] = 2025 - df['Year_Birth']
df = df.drop(columns=['Year_Birth'])

# Fill missing numeric values with median
df = df.fillna(df.median(numeric_only=True))

# One-hot encode Education and Marital_Status
df = pd.get_dummies(df, columns=['Education', 'Marital_Status'], drop_first=True)

# Total spending feature
spending_cols = [c for c in df.columns if c.startswith('Mnt')]
df['Total_Spending'] = df[spending_cols].sum(axis=1)

# Select features for clustering
features = ['Age', 'Income', 'Kidhome', 'Teenhome', 'Recency', 'Complain',
            'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',
            'NumStorePurchases', 'NumWebVisitsMonth', 'Total_Spending'] + \
           [c for c in df.columns if c.startswith('Education_') or c.startswith('Marital_Status_')]

df_features = df[features]

# Scale features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df_features)

st.write("### Features after scaling")
st.dataframe(pd.DataFrame(scaled_features, columns=features).head())

# Apply PCA to reduce dimensions to 2 for visualization
pca = PCA(n_components=2, random_state=42)
pca_features = pca.fit_transform(scaled_features)

st.write(f"Explained variance by PCA components: {pca.explained_variance_ratio_}")

# Plot PCA scatter before clustering
plt.figure(figsize=(8,6))
sns.scatterplot(x=pca_features[:,0], y=pca_features[:,1])
plt.title('PCA Scatter Plot (2 Components)')
st.pyplot(plt.gcf())
plt.clf()

# Perform hierarchical clustering on scaled data (not PCA)
linked = linkage(scaled_features, method='ward')

# Plot dendrogram (truncated for clarity)
plt.figure(figsize=(10,5))
dendrogram(linked, truncate_mode='level', p=5)
plt.title('Hierarchical Clustering Dendrogram (truncated)')
plt.xlabel('Sample index or cluster size')
plt.ylabel('Distance')
st.pyplot(plt.gcf())
plt.clf()

# Select number of clusters
n_clusters = st.slider("Select number of clusters", 2, 10, 4)

# Get cluster labels
cluster_labels = fcluster(linked, t=n_clusters, criterion='maxclust')
df['Cluster'] = cluster_labels

st.write("### Cluster counts")
st.write(df['Cluster'].value_counts().sort_index())

st.write("### Cluster summary (mean values)")
st.write(df.groupby('Cluster')[features].mean().round(2))

# Plot PCA scatter colored by cluster
plt.figure(figsize=(8,6))
sns.scatterplot(x=pca_features[:,0], y=pca_features[:,1], hue=cluster_labels, palette='Set2', s=50)
plt.title('PCA Scatter Plot colored by Cluster')
st.pyplot(plt.gcf())
plt.clf()